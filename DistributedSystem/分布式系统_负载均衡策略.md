- [分布式系统的负载均衡策略](#分布式系统的负载均衡策略)
	- [1. 基本概念](#1-基本概念)
		- [1.1. 负载均衡算法](#11-负载均衡算法)
		- [1.2. 节点负载衡量](#12-节点负载衡量)
		- [1.3. 均衡的目标](#13-均衡的目标)
	- [2. Random 负载均衡算法](#2-random-负载均衡算法)
	- [3. 简单 hash 负载均衡算法](#3-简单-hash-负载均衡算法)
	- [4. 一致性 hash 负载均衡算法](#4-一致性-hash-负载均衡算法)
	- [4. hash 对照表负载均衡算法](#4-hash-对照表负载均衡算法)
	- [5. Refer Links](#5-refer-links)

# 分布式系统的负载均衡策略

todo: 
<!-- 
http://km.oa.com/group/19135/articles/show/214816?kmref=search&from_page=1&no=1

http://km.oa.com/group/628/articles/show/151673?kmref=related_post

http://km.oa.com/group/1004/articles/show/315859?kmref=related_post

http://km.oa.com/group/669/articles/show/174836?kmref=related_post -->

## 1. 基本概念

### 1.1. 负载均衡算法

目前，负载均衡算法不管是在学术研究还是在工程实现上都已比较成熟，算法大体可分以下几种：
- 随机（random）算法
- 轮询（round-robin）算法
- 哈希（hash）算法
- 一致性哈希算法
- 静态权重调度算法
- 动态权重、自适应算法

### 1.2. 节点负载衡量

根据不同类型的系统应用，通常会定义不同的度量方式，如：CPU 资源、内存资源、当前进程数、吞吐量、成功率、响应时间等指标都可以作为负载度量的指标，各个指标的重要程度根据实际情况有所不同。

### 1.3. 均衡的目标

一个负载均衡算法，至少需要实现以下目标：
- 可维护型：把迁移代价控制在可接受范围内。
- 灵活性：可以对特定服务器的请求负载进行细粒度控制，以支持版本的灰度变更。
- 可用性：支持机架、IDC 容灾。

## 2. Random 负载均衡算法

在提供海量用户访问的服务器集中做负载均衡，Random 方法应该是最为简单的。用户每个请求到来，都会在集群中随机选择一台服务器对其服务。

- 优点：

	简单、快速，不需要维护复杂的路由规则和路由信息，也不需要专门部署一套硬件级的负载均衡设备，可以很好地控制成本。

- 缺点：
	- 只适合无状态服务的场景，对于带有诸如 cache 服务模块等有状态服务的集群节点，会严重影响整个集群的服务效率。对于 cache 来讲，因为是随机选择节点，同一个请求有时候会落到 A 机器，有时会落到 B 机器，导致 cache 数据频繁地被淘汰，命中率非常低下，严重影响到用户的体验。
	- 对于版本变更灰度上线也非常不利，由于随机负载均衡的随机性，导致后端所有节点接受请求的概率是相等的，无法控制特点节点的请求个数，缺乏灵活性。

## 3. 简单 hash 负载均衡算法

简单 Hash 负载均衡方法是对 Random 方法的一种改进，他的主要原理是对查询 key 进行哈希计算，按照机器数量进行取余，选取中的那台机器提供服务。

- 优点：

	这种方法也很简单，数据分散性也相当优秀，而且同一个请求 key 会落到同一台机器上，对于要做 cache 的机器来说，cache 命中率问题可以得到很好的解决。

- 缺点：
	- 遇到机器扩容或者节点移除等机器数量发生变化的情况，需要对缓存进行重构，cache 命中率会瞬间达到一个非常低的水平，大部分请求突然压到后端的存储节点上，导致后端物理存储节点瞬间奔溃。
	- Hash 方法在对集群中的物理存储节点进行负载均衡的时候，也会带来巨大的迁移代价。比如某个数据根据 key 做 hash 被分配到 A 机器进行物理落地存储，此时需要对存储节点进行扩容，机器数量增加。下次通过同一个 key 去访问对应的数据，请求就不会落到 A 机器。因此，在进行存储扩容的时候，大量的数据迁移成为必须。
	- 无法解决版本变更、灰度发布的问题。

## 4. 一致性 hash 负载均衡算法

一致性 hash 算法又是对简单 hash 方案的一次重大改进，用来解决简单 hash 算法节点数量变化带来的迁移代价过大问题。

1. 该方法首先求出服务器（节点）的哈希值，并将其配置到 0～2^32 的圆上。然后用同样的方法求出存储数据的键的哈希值，并映射到圆上。
1. 然后从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个服务器上。如果超过 2^32 仍然找不到服务器，就会保存到第一台服务器上。
1. 当要往集群中添加一个节点的时候，我们只需要把原来落在 node2 和 node5 之间的数据从 node4 搬迁到 node5 就行了，相反，如果要从集群移除一个节点，那么映射该范围的请求就会被 node4 接管。

![image](http://img.cdn.firejq.com/jpg/2018/7/20/76a96b078f27cc3680bfceba05de0fb2.jpg)

- 优点：
	- 根据一致性 hash 原理，节点越多，每次扩容和移除机器，带来的迁移代价会越小。
- 缺点：
	- 无法解决版本灰度变更的问题。

## 4. hash 对照表负载均衡算法

在实行负载均衡策略的机器上维护如下的一个 hash 对照表，第一列是用用户请求的 key 计算得的 hash 值，另一列是负责这个 hash 所对应的服务器节点。

![image](http://img.cdn.firejq.com/jpg/2018/7/20/af84302c3c9dca7f170160c16df45fa0.jpg)

该对照表包含 10000 个格子，格子的个数可以根据集群规模进行设定，一般远大于实际物理节点的个数。格子的个数一旦确定，上线之后就不能修改，因为增加格子的个数，就要彻底重构对照表，映射关系会完全打乱，因此带来巨大的迁移代价。一般 10000 以上的格子已经足够可以应付集群的扩容了。

假设现在需要对具有 5 个服务器节点的集群进行负载均衡，总共有 10000 个格子，则每台机器负责 2000 个格子。客户端将 key 进行 hash 后，对 10000 进行取模，就可以找到负责的数据节点 IP，然后与其直接通信，即可把用户请求均衡分布到 5 个不同的物理节点上。而且集群的负载均衡策略问题，最终由 hash 对照表的格子分布策略来决定，如果需要对集群的负载均衡策略调整，只需要调整对照表就可以了，让工作变得非常简单和灵活。

- 增加 / 删除节点

	假设要在 2 个节点的集群中再扩容一台机器 192.168.10.3，只需将原来负责 4、5 格子的请求现在由 192.168.10.3 这台机器来接管，尽量保持原有的对照关系。此时我们需要把扩容之前落在 4、5 格子上的部分数据从 192.168.10.1 和 192.168.10.2 上迁移到新增节点上，使得扩容之前的数据通过新增节点也可以顺利访问到。

	![image](http://img.cdn.firejq.com/jpg/2018/7/20/54bc83279866602f89fba38b6f2b1989.jpg)

	节点移除的情况，只需要把上述过程反过来。

- 灰度

	正常情况下，我们希望集群中的每个节点被选中提供服务的概率是相等的。只要保证每个节点在对照表中对应的格子数相等就可以了。如果新版本上线需要进行灰度、逐步放量，我们只需要控制该服务器对应的格子个数就可以了。一个格子对应 1/10000 的请求量，10 个对应 1/1000,100 对应 1/100,.......，整个新版本的上线过程是非常可控的。

- 容灾

	![image](http://img.cdn.firejq.com/jpg/2018/7/20/5877a9f1b5e0d254fc427930259b8fd9.jpg)

	如果数据有多个备份，那么对照表就包含多列，比如备份是为 3，则表有 4 列，后面的 3 列都是数据存储节点，存储节点可以位于不同的机架后者 IDC。第二列可以作为主节点信息，之后的列都作为辅节点信息，每次 hash 取模到特定的格子，先由主节点提供服务，主节点一旦失效，就把之后的辅节点升级为主节点。

## 5. Refer Links