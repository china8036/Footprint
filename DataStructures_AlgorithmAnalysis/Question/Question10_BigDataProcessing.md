- [海量数据处理问题](#海量数据处理问题)
  - [1. 基本概念](#1-基本概念)
  - [2. 解决方法](#2-解决方法)
    - [2.1. 分而治之 (Hash)](#21-分而治之-hash)
    - [2.2. Bloom Filter](#22-bloom-filter)
    - [2.3. Bit-map](#23-bit-map)
    - [2.4. Heap](#24-heap)
    - [2.5. 双层桶划分](#25-双层桶划分)
    - [2.6. 数据库索引](#26-数据库索引)
    - [2.7. 倒排索引](#27-倒排索引)
    - [2.8. 外排序](#28-外排序)
    - [2.9. Trie 树](#29-trie-树)
    - [2.10. 分布式处理](#210-分布式处理)
  - [3. 常见问题](#3-常见问题)
    - [3.1. Top K 问题](#31-top-k-问题)
    - [3.2. 相同字符串问题](#32-相同字符串问题)
    - [3.3. 中位数问题](#33-中位数问题)
    - [3.4. 去重问题](#34-去重问题)
    - [3.5. 无重复个数](#35-无重复个数)
  - [4. Refer Links](#4-refer-links)

# 海量数据处理问题

## 1. 基本概念

海量数据处理问题就是基于海量数据上的存储、处理、操作。何谓海量，就是数据量太大，所以导致要么是无法在较短时间内迅速解决，要么是数据太大，导致无法一次性装入内存。

## 2. 解决方法

原则：
- 针对时间：**可以采用巧妙的算法搭配合适的数据结构，如 Bloom filter / Hash / bit-map / 堆 / 数据库或倒排索引 / Trie 树**。
- 针对空间：**大而化小，分而治之（Hash 映射），将规模大化为规模小的，再各个击破**。

一般方法：
- Hash
- Bloom Filter
- Bit-map
- Heap
- 双层桶划分
- 数据库索引
- 倒排索引 (Inverted Index)
- Trie 树
- 外排序
- 分布式处理 Hadoop/Mapreduce

### 2.1. 分而治之 (Hash)

对于海量数据而言，由于无法一次性装进内存处理，因此可以使用分治的思想：通过 Hash 映射的方式把海量数据分割成相应的小块数据，然后针对各个小块数据进行统计（如 HashMap）或其它操作（如排序），再将处理结果进行归并即可。

- 例 1：在网络运营商的海量日志数据中，提取出某日访问百度次数最多的那个 IP。

  由于每天访问百度的 IP 数量巨大，如果想一次性把所有 IP 数据装进内存处理，则内存容量明显不够，故针对数据太大，内存受限的情况，可以把大文件转化成（取模映射）小文件，从而大而化小，逐个处理。

  具体分为以下 3 个步骤：
  1. 分而治之 /hash 映射：首先把这一天访问百度日志的所有 IP 提取出来，然后逐个写入到一个大文件中，接着采用映射的方法，比如 %1000，把整个大文件映射为 1000 个小文件。
  1. HashMap 统计：当大文件转化成了小文件，那么我们便可以采用 hash_map(ip, value) 来分别对 1000 个小文件中的 IP 进行频率统计，再找出每个小文件中出现频率最大的 IP。
  1. 堆 / 快速 / 归并排序：统计出 1000 个频率最大的 IP 后，依据各自频率的大小进行排序（可采取堆排序)，找出那个频率最大的 IP，即为所求。

  NOTE: Hash 取模是一种等价映射，不会存在同一个元素分散到不同小文件中去的情况，即这里采用的是 %1000 算法，那么同一个 IP 在 hash 后，只可能落在同一个文件中。

- 例 2：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为 1-255 字节。假设目前有一千万个查询字符串（但去重后总数不超过 3 百万个），请统计最热门的 10 个查询串，要求使用的内存不能超过 1G。

  根据题目描述，虽然有一千万个 Query，但是由于重复度比较高，去除重复后，事实上只有 300 万的 Query，每个 Query 大小为 255 Byte，因此可以将其全部放进内存（300 万个没有重复的字符串，都是最大长度，占用内存 3M * 1K / 4 = 0.75G < 1G）。
  
  即省略分而治之 / hash 映射的步骤，直接进行 hashMap 统计，然后使用大小为 10 的最小堆提取 Top K 即可。当然也可以采用 Trie 树，关键字域存该查询串出现的次数，没有出现为 0，最后用 10 个元素的最小推来对出现频率进行排序即可。

  但若题目描述为可用内存无法同时容纳全部无重复的字符串，则需要顺序读取所有记录，依次进行 hash 取模操作，从而将记录分成多个小文件后再进行下一步词频统计操作。当然，如果其中有的小文件超过了内存限制的大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过内存限制。

- 例 3：海量数据分布在 100 台电脑中，想个办法高效统计出这批数据的 TOP10。

  方法一（暴力求解）：直接统计统计每台电脑中各个元素的出现次数，然后把同一个元素在不同机器中的出现次数相加，最终从所有数据中找出 TOP 10。

  方法二（重新取模）：遍历一遍所有数据，重新 hash 取摸，如此使得同一个元素只出现在单独的一台电脑中，然后采用上面所说的方法，统计每台电脑中各个元素的出现次数找出 TOP 10，继而组合 100 台电脑上的 TOP 10，找出最终的 TOP 10。

- 例 4：给定 a、b 两个文件，各存放 50 亿个 url，每个 url 各占 64 字节，内存限制是 4G，让你找出 a、b 文件共同的 url？

  可以估计每个文件的大小为 5G×64=320G，远远大于内存限制的 4G。所以不可能将其完全加载到内存中处理，考虑采取分而治之的方法。

  1. 分而治之 /hash 映射
     
      遍历文件 a，对每个 url 求取 hash(url) % 1000，然后根据所取得的值将 url 分别存储到 1000 个小文件中，记为 a0a1a2...a999。这样每个小文件的大约为 300M。
      
      遍历文件 b，采取和 a 相同的方式将 url 分别存储到 1000 小文件中，记为 b0b1b2...b999。
      
      这样处理后，所有可能相同的 url 都在对应的小文件（a0 和 b0、a1 和 b1）中，不对应的小文件不可能有相同的 url。
  
  1. hash_set 统计
     
      经过第一步操作后，我们只要求出 1000 对小文件中相同的 url 即可。

      求每对小文件中相同的 url 时，可以把其中一个小文件的 url 存储到 hashSet 中。然后遍历另一个小文件的每个 url，看其是否在刚才构建的 hashSet 中，如果是，那么就是共同的 url，存到文件里面就可以了。

### 2.2. Bloom Filter

Bloom Filter 是一种空间效率很高的随机数据结构，它利用位数组很简洁地表示一个集合，并能判断一个元素是否属于这个集合。

Bloom Filter 的这种高效是有一定代价的：在判断一个元素是否属于某个集合时，有可能会把不属于这个集合的元素误认为属于这个集合（false positive）。因此，Bloom Filter 不适合那些“零错误”的应用场合。而在能容忍低错误率的应用场合下，采用 Bloom Filter 的数据结构，可以通过极少的错误换取了存储空间的极大节省。 

- 例 1：给你 A,B 两个文件，各存放 50 亿条 URL，每条 URL 占用 64 字节，内存限制是 4G，让你找出 A,B 文件共同的 URL。如果是三个乃至 n 个文件呢？

  如果允许有一定的错误率，可以使用 Bloom filter，4G 内存大概可以表示 340 亿 bit。将其中一个文件中的 url 使用 Bloom filter 映射为这 340 亿 bit，然后挨个读取另外一个文件的 url，检查是否与 Bloom filter，如果是，那么该 url 应该是共同的 url（注意会有一定的错误率）。

### 2.3. Bit-map

所谓的 Bit-map 就是用一个 bit 位来标记某个元素对应的 Value，而 Key 即是该元素。由于采用了 Bit 为单位来存储数据，因此在存储空间方面，可以大大节省。

- 例 1: 

  已知某个文件内包含一些电话号码，每个号码为 8 位数字，统计不同号码的个数。

  8 位最多 99 999 999，大概需要 99m 个 bit，大概 10 几 m 字节的内存即可。 （可以理解为从 0-99 999 999 的数字，每个数字对应一个 Bit 位，所以只需要 99M 个 Bit==12.4MBytes，这样，就用了小小的 12.4M 左右的内存表示了所有的 8 位数的电话）

- 例 2: 

  2.5 亿个整数中找出不重复的整数的个数，内存空间不足以容纳这 2.5 亿个整数。

  采用 2-Bitmap（每个数分配 2bit，00 表示不存在，01 表示出现一次，10 表示多次，11 无意义）进行，共需内存 2^32 * 2 bit=1 GB 内存，还可以接受。然后扫描这 2.5 亿个整数，查看 Bitmap 中相对应位，如果是 00 变 01，01 变 10，10 保持不变。扫描完事后，查看 bitmap，把对应位是 01 的整数输出即可。

- 例：

  给 40 亿个不重复的 unsigned int 的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那 40 亿个数当中？

  采用与例 2 相同的思路即可。

### 2.4. Heap

堆是一种特殊的二叉树，具备以下两种性质：
- 每个节点的值都大于（或者都小于，称为最小堆）其子节点的值。
- 树是完全平衡的，并且最后一层的树叶都在最左边。

最大 / 小堆一般可用于解决 Top K 问题。

- 例 1：在 1 千万个数中找最大的前 100 个数。

  用一个 100 个元素大小的最小堆即可。

### 2.5. 双层桶划分

面对一堆大量的数据我们无法利用直接寻址表进行处理的时候，我们可以将其分成一个个小的单元，然后根据一定的策略来处理这些小单元，从而达到目的。这也就是双层桶划分的思想，通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行。可以通过多次缩小，双层只是一个例子，分治才是其根本（只是“只分不治”）。

- 例 1：在 100 亿个整数中找到其中位数，已知内存空间不足以容纳所有整数。

  思路：把所有数划分到各个小区间，把每个数映射到对应的区间里，对每个区间中数的个数进行计数，数一遍各个区间，看看中位数落在哪个区间，若够小，使用基于内存的算法，否则继续划分。

  若 int 是 32 位的，根据每个整数的二进制前 5 位，划分为 32 个桶，把数放进对应桶中。如果该桶放不下，继续划分，直至内存可以放心为止。统计每个桶中元素个数，算出中位数一定出现在哪个桶中，而且计算出是该桶中的第几大。

- 例 2：在 2.5 亿个整数中找出不重复的整数的个数，已知内存空间不足以容纳所有整数。

  有点像鸽巢原理，整数个数为 2^32, 也就是，我们可以将这 2^32 个数，划分为 2^8 个区域（比如用单个文件代表一个区域)，然后将数据分离到不同的区域，然后不同的区域在利用 bitmap 就可以直接解决了。也就是说只要有足够的磁盘空间，就可以很方便的解决。 当然这个题也可以用我们前面讲过的 BitMap 方法解决。

- 例 3：有一个 0-30000 的随机数生成器。请根据这个随机数生成器，设计一个抽奖范围是 0-350000 彩票中奖号码列表，其中要包含 20000 个中奖号码。
  
  <!-- todo: -->
  
  这个题刚好和上面两个思想相反，一个 0 到 3 万的随机数生成器要生成一个 0 到 35 万的随机数。那么我们完全可以将 0-35 万的区间分成 35/3=12 个区间，然后每个区间的长度都小于等于 3 万，这样我们就可以用题目给的随机数生成器来生成了，然后再加上该区间的基数。那么要每个区间生成多少个随机数呢？计算公式就是：`区间长度*随机数密度`，在本题目中就是 30000*（20000/350000）。
  
  最后要注意一点，该题目是有隐含条件的：彩票，这意味着你生成的随机数里面不能有重复，这也是为什么用双层桶划分思想的另外一个原因。

### 2.6. 数据库索引

索引是对数据库表中一列或多列的值进行排序的一种结构，建立索引的目的是加快对数据库中表的记录的查找或排序。

### 2.7. 倒排索引

倒排索引（Inverted index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。它是文档检索系统中最常用的数据结构，常被应用于搜索引擎和关键字查询的问题中。

正向索引开发出来用来存储每个文档的单词的列表。正向索引的查询往往满足每个文档有序频繁的全文查询和每个单词在校验文档中的验证这样的查询。在正向索引中，文档占据了中心的位置，每个文档指向了一个它所包含的索引项的序列。也就是说文档指向了它包含的那些单词，而反向索引则是单词指向了包含它的文档，很容易看到这个反向的关系。

例 1：实现一个文档检索系统，查询哪些文件中包含了某单词，比如常见的学术论文的关键字搜索。

### 2.8. 外排序

所谓外排序，顾名思义，即是在内存外面的排序，因为当要处理的数据量很大，而不能一次装入内存时，此时只能放在读写较慢的外存储器（通常是硬盘）上。外排序通常采用的是一种“排序 - 归并”的策略。

- 例 1：

  给定一个文件，里面最多含有 n 个不重复的正整数（也就是说可能含有少于 n 个不重复正整数），且其中每个数都小于等于 n，n=10^7。 输出：得到按从小到大升序排列的包含所有输入的整数的列表。 条件：最多有大约 1MB 的内存空间可用，但磁盘空间足够。且要求运行时间在 5 分钟以下，10 秒为最佳结果。

  采用多路归并排序的方法，分而治之，大而化小。也就是把整个大文件分为若干大小的几块，然后分别对每一块进行排序，最后完成整个过程的排序。k 趟算法可以在 kn 的时间开销内和 n/k 的空间开销内完成对最多 n 个小于 n 的无重复正整数的排序。

  比如可分为 2 块（k=2，1 趟反正占用的内存只有 1.25/2M），1~4999999，和 5000000~9999999。先遍历一趟，首先排序处理 1~4999999 之间的整数（用 5000000/8=625000 个字的存储空间来排序 0~4999999 之间的整数），然后再第二趟，对 5000001~1000000 之间的整数进行排序处理。

### 2.9. Trie 树

<!-- todo: -->

Trie 树，即字典树，又称单词查找树或键树，是一种树形结构。典型应用是用于统计和排序大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是最大限度地减少无谓的字符串比较，查询效率比较高。

Trie 的核心思想是空间换时间，利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。

- 例 1：

  一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前 10 个词，请给出思想，给出时间复杂度分析。

  用 Trie 树统计每个词出现的次数，时间复杂度是 O(nle)（le 表示单词的平均长度），然后是找出出现最频繁的前 10 个词。当然，也可以用堆来实现，时间复杂度是 O(nlg10)。所以总的时间复杂度，是 O(nle) 与 O(nlg10) 中较大的哪一个。

- 例 2：

  搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为 1-255 字节。假设目前有一千万个查询字符串（但去重后总数不超过 3 百万个），请统计最热门的 10 个查询串，要求使用的内存不能超过 1G。

  利用 Trie 树，关键字域存该查询串出现的次数，没有出现为 0。最后用 10 个元素的最小推来对出现频率进行排序。

### 2.10. 分布式处理

https://www.kancloud.cn/kancloud/the-art-of-programming/41616

https://blog.csdn.net/v_july_v/article/details/6704077

## 3. 常见问题

### 3.1. Top K 问题

有一个 1G 大小的一个文件，里面每一行是一个词，词的大小不超过 16 字节，内存限制大小是 1M。返回频数最高的 100 个词。

- 方案一：分而治之 / Hash 映射 + HashMap 统计 + 堆 / 快速 / 归并排序
  1. 分而治之 / Hash 映射
    
      顺序读取文件，对于每个词 x，取 hash(x)%5000，然后把该值存到 5000 个小文件（记为 x0,x1,...x4999）中。这样每个文件大概是 200k 左右。当然，如果其中有的小文件超过了 1M 大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过 1M。

  1. hash_map 统计
    
      对每个小文件，采用 trie 树 /hash_map 等统计每个文件中出现的词以及相应的频率。

  1. 堆 / 归并排序
    
      取出出现频率最大的 100 个词（可以用含 100 个结点的最小堆）后，再把 100 个词及相应的频率存入文件，这样又得到了 5000 个文件。最后就是把这 5000 个文件进行归并（类似于归并排序）的过程了。

- 方案二：Bit-map

类似问题：
- 海量日志数据，提取出某日访问百度次数最多的那个 IP。
- 搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为 1-255 字节。假设目前有一千万个查询字符串（但去重后总数不超过 3 百万个），请统计最热门的 10 个查询串，要求使用的内存不能超过 1G。

### 3.2. 相同字符串问题

给定 a、b 两个文件，各存放 50 亿个 url，每个 url 各占 64 字节，内存限制是 4G，让你找出 a、b 文件共同的 url？

- 方案 1：分而治之 / Hash 映射 + HashMap 统计 + 堆 / 快速 / 归并排序
  
  可以估计每个文件安的大小为 5G×64=320G，远远大于内存限制的 4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。

  遍历文件 a，对每个 url 求取 hash(url)%1000，然后根据所取得的值将 url 分别存储到 1000 个小文件（记为 a0,a1,...,a999）中。这样每个小文件的大约为 300M。

  遍历文件 b，采取和 a 相同的方式将 url 分别存储到 1000 小文件（记为 b0,b1,...,b999）。这样处理后，所有可能相同的 url 都在对应的小文件（a0vsb0,a1vsb1,...,a999vsb999）中，不对应的小文件不可能有相同的 url。然后我们只要求出 1000 对小文件中相同的 url 即可。

  求每对小文件中相同的 url 时，可以把其中一个小文件的 url 存储到 hash_set 中。然后遍历另一个小文件的每个 url，看其是否在刚才构建的 hash_set 中，如果是，那么就是共同的 url，存到文件里面就可以了。

- 方案 2：Bloom filter
  
  如果允许有一定的错误率，可以使用 Bloom filter，4G 内存大概可以表示 340 亿 bit。将其中一个文件中的 url 使用 Bloom filter 映射为这 340 亿 bit，然后挨个读取另外一个文件的 url，检查是否与 Bloom filter，如果是，那么该 url 应该是共同的 url（注意会有一定的错误率）。

### 3.3. 中位数问题

[100 亿个 int 整数，内存足够，如何找到中位数？内存不足，如何找到中位数？](https://www.nowcoder.com/questionTerminal/359d6869d5ce4738bf9c9a42b67d9568)

- 当内存足够时：

  采用快排 partition 的思想，找到第 n 大的数，n 为 50 亿：
  1. 随机选取一个数，将比它小的元素放在它左边，比它大的元素放在右边 。
  1. 如果它恰好在中位数的位置，那么它就是中位数，直接返回 。
  1. 如果小于它的数超过一半，那么中位数一定在左半边，递归到左边处理（还是第几大） 。
  1. 否则中位数一定在右半边，根据左半边的元素个数计算出中位数是右半边的第几大（重新算第几大），然后递归到右半边处理。

- 当内存不足时：

  - 方法⼀：⼆分法 

    由于数据都是 int 整数，因此在 32 位系统中大小范围是 [-2^32, 2^32- 1]， 有了范围我们就可以对这个范围进行二分。找有多少个数⼩于 Mid, 多少数大于 mid，然后递归，和基于 quicksort 思想的第 k 大⽅方法类似。

  - 方法二：分桶法

    把所有数划分到各个小区间，把每个数映射到对应的区间里，对每个区间中数的个数进行计数，数一遍各个区间，看看中位数落在哪个区间，若够小，使用基于内存的算法，否则继续划分。

    假设整数是 32 位的，根据每个整数二进制前的 5 位，可以划分为 32 个不同的桶，如果某个桶的个数在内存中放不下，则继续划分，直到内存可以放下为止；然后统计每个桶中的数的个数，就可以中位数一定出现在哪个桶中，而且知道是该桶中第几大数，因为桶的划分是根据二进制前缀来进行划分的，桶之间是排好序的。
    
  - 方法三：Bit-map

    考虑 32 位无符号整形 int，最大为 2 的 32 次方，给予每一位 2 个 bit，00 表示为出现，01 表示出现 1 次，10 表示出现多次（之后不在出现变化），那么所需的内存为 2 的 32 次方*2bit 等于 1GB 左右的内存来表示所有数，顺序便利 100 亿个数，那么一个位数组的每一位都可能从 00 变为 01，再从 01 变为 10，之后过滤调位数组中表示为 00 的位置，取其中间下标即为中位数。

### 3.4. 去重问题

1000 万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。

提示：这题用 Trie 树比较合适，hash_map 也行。当然，也可以先 hash 成小文件分开处理再综合。

### 3.5. 无重复个数

在 2.5 亿个整数中找出不重复的整数，注，内存不足以容纳这 2.5 亿个整数。

- 方案 1：采用 2-Bitmap（每个数分配 2bit，00 表示不存在，01 表示出现一次，10 表示多次，11 无意义）进行，共需内存 2^32 * 2 bit=1 GB 内存，还可以接受。然后扫描这 2.5 亿个整数，查看 Bitmap 中相对应位，如果是 00 变 01，01 变 10，10 保持不变。所描完事后，查看 bitmap，把对应位是 01 的整数输出即可。

- 方案 2：采用分治方法，即对 2.5 亿个数进行划分。然后在小区域中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。

## 4. Refer Links

[v_july_v: 海量数据处理专题](https://blog.csdn.net/v_july_v/article/category/1106578)

[帝都码农：海量数据处理专题](http://diducoder.com/tag/%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE)