

# Apache Arrow

https://github.com/apache/arrow

https://arrow.apache.org/docs/

用户在应用大数据分析时除了将Hadoop等大数据平台作为一个经济的存储和批处理平台之外也很看重分析系统的扩展性和性能。过去几年开源社区已经发布了很多工具来完善大数据分析的生态系统，这些工具涵盖了数据分析的各个层面，比如列式存储格式(Parquet/ORC)、内存计算层(Drill、Spark、Impala和Storm)以及强大的API接口(Python和R语言)。Arrow则是最新加入的一员，它提供了一种跨平台跨应用的内存数据交换格式。

在分布式系统内部，每个系统都有自己的内存格式，大量的 CPU 资源被消耗在序列化和反序列化过程中，并且由于每个项目都有自己的实现，没有一个明确的标准，造成各个系统都在重复着复制、转换工作，这种问题在微服务系统架构出现之后更加明显，Arrow 的出现就是为了解决这一问题。作为一个跨平台的数据层，我们可以使用 Arrow 加快大数据分析项目的运行速度。

需要明确的是，Apache Arrow 不是一个引擎，也不是一个存储系统，它是用来处理分层的列式内存数据的一系列格式和算法。它不是一个独立的软件，而是系统中用来加速数据分析的一个组件。很多开源项目都已经支持了 Arrow，而且其他商业化的项目也有这个趋势。对于已经支持 Arrow 的项目来说，这些项目不再需要序列化和反序列化各种数据，从而以极小的成本来共享数据资源，对于同一集群下的系统则完全不需要进行任何的数据格式转换。

Apache Arrow 之所以会流行，是因为它不针对特定产品，而是可以为大数据整个生态系统带来便利。有了 Arrow 作为标准数据交换格式，各个数据分析系统和应用之间的交互性有了全新的方式，我们不再需要把 CPU 资源花费在数据的序列化和反序列化上了，实现了不同系统之间数据的无缝连接，官网发表的文章显示，它的目标是提升数据之间的 100 倍交换速度，这样才能真正对数据分析流程进行加速。

提高大数据分析性能的一个重要手段是对列式数据的设计和处理。列式数据处理借助向量计算和SIMD使我们可以充分挖掘硬件的潜力。Apache Drill这一大数据查询引擎无论是在硬盘还是在内存中数据都是以列的方式存在的，而Arrow就是由Drill中Value Vector这一数据格式发展而来。除了列式数据，Apache Arrow也支持关系型和动态数据集，这使它成了处理物联网等数据时的理想格式选择。

https://www.infoq.cn/article/apache-arrow

https://www.cnblogs.com/smartloli/p/6367719.html

http://www.idataskys.com/2019/01/21/pyarrow%E5%88%9D%E6%8E%A2/
