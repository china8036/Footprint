- [惊群效应](#惊群效应)
    - [1. 基本概念](#1-基本概念)
    - [2. 不良影响](#2-不良影响)
    - [3. 发生场景](#3-发生场景)
    - [4. 解决措施](#4-解决措施)
    - [5. Refer Links](#5-refer-links)

# 惊群效应

## 1. 基本概念

## 2. 不良影响

- 上下文切换成本

- 临界资源争抢成本

## 3. 发生场景

- `accept()`

  在 Linux 2.6 版本以后，Linux 内核已经解决了 accept() 函数的“惊群”现象，大概的处理方式就是，当内核接收到一个客户连接后，只会唤醒等待队列上的第一个进程（线程）, 所以**如果服务器采用 accept 阻塞调用方式，在最新的 linux 系统中已经没有“惊群效应”了**。

- `epoll_wait()`

  epoll_wait 的惊群确实存在。为什么内核处理了 accept 的惊群，却不处理 epoll_wait 的惊群呢？

  accept() 确实只应该被一个进程调用成功，内核很清楚这一点。但 epoll_wait 不一样，他监听的文件描述符，除了可能后续被 accept 调用外，还有可能是其他网络 IO 事件的，而其他 IO 事件是否只能由一个进程处理，是不一定的，内核不能保证这一点，这是一个由用户决定的事情，例如可能一个文件会由多个进程来读写。因此，**对 epoll 的惊群，内核则不予处理**。

- `pthread_cond_broadcast()`

  多线程环境下，**使用 pthread_cond_broadcast 来唤醒 wait 的线程时，会导致惊群现象的发生**。

## 4. 解决措施

- 加锁

- nginx 解决方案
  ```cpp
  void ngx_process_events_and_timers(ngx_cycle_t *cycle)
  {
      // ... ...
      // 是否通过对 accept 加锁来解决惊群问题，需要工作线程数大于 1 且配置文件打开 accetp_mutex
      if (ngx_use_accept_mutex) {
          // 超过配置文件中最大连接数的 7/8 时，该值大于 0，此时满负荷不会再处理新连接，简单负载均衡
          if (ngx_accept_disabled > 0) {
              ngx_accept_disabled--;
          } else {
              // 多个 worker 仅有一个可以得到这把锁。获取锁不会阻塞过程，而是立刻返回，获取成功的话
              // ngx_accept_mutex_held 被置为 1。拿到锁意味着监听句柄被放到本进程的 epoll 中了，如果
              // 没有拿到锁，则监听句柄会被从 epoll 中取出。
              if (ngx_trylock_accept_mutex(cycle) == NGX_ERROR) {
                  return;
              }
              if (ngx_accept_mutex_held) {
                  // 此时意味着 ngx_process_events() 函数中，任何事件都将延后处理，会把 accept 事件放到
                  // ngx_posted_accept_events 链表中，epollin|epollout 事件都放到 ngx_posted_events 链表中
                  flags |= NGX_POST_EVENTS;
              } else {
                  // 拿不到锁，也就不会处理监听的句柄，这个 timer 实际是传给 epoll_wait 的超时时间，修改
                  // 为最大 ngx_accept_mutex_delay 意味着 epoll_wait 更短的超时返回，以免新连接长时间没有得到处理
                  if (timer == NGX_TIMER_INFINITE || timer > ngx_accept_mutex_delay) {
                      timer = ngx_accept_mutex_delay;
                  }
              }
          }
      }
      // ... ...
      (void) ngx_process_events(cycle, timer, flags);   // 实际调用 ngx_epoll_process_events 函数开始处理
      // ... ...
      if (ngx_posted_accept_events) { // 如果 ngx_posted_accept_events 链表有数据，就开始 accept 建立新连接
          ngx_event_process_posted(cycle, &ngx_posted_accept_events);
      }

      if (ngx_accept_mutex_held) { // 释放锁后再处理下面的 EPOLLIN EPOLLOUT 请求
          ngx_shmtx_unlock(&ngx_accept_mutex);
      }

      if (delta) {
          ngx_event_expire_timers();
      }

      ngx_log_debug1(NGX_LOG_DEBUG_EVENT, cycle->log, 0, "posted events %p", ngx_posted_events);
      // 然后再处理正常的数据读写请求。因为这些请求耗时久，所以在 ngx_process_events 里 NGX_POST_EVENTS 标志将事件都放入 ngx_posted_events 链表中，延迟到锁释放了再处理。
  }}
  ```

- `SO_REUSEPORT` 参数

  Linux 内核的 3.9 版本带来了 `SO_REUSEPORT` 特性，该特性支持多个进程或者线程绑定到同一端口，提高服务器程序的性能，允许多个套接字 bind() 以及 listen() 同一个 TCP 或 UDP 端口，并且在内核层面实现负载均衡。

  在未开启 SO_REUSEPORT 的时候，由一个监听 socket 将新接收的连接请求交给各个工作者处理，看图示：

  ![image](http://img.cdn.firejq.com/jpg/2018/8/22/edfebae33c3b4f293932b19818b3ed1b.jpg)

  在使用 SO_REUSEPORT 后，多个进程可以同时监听同一个 IP：端口，然后由内核决定将新链接发送给哪个进程，显然会降低每个工人接收新链接时锁竞争：

  ![image](http://img.cdn.firejq.com/jpg/2018/8/22/b6826d932875c8f659b600fb091a8887.jpg)

## 5. Refer Links

TODO:

[linux 惊群效应](https://blog.csdn.net/lyztyycode/article/details/78648798)