- [JVM 性能调优实例](#jvm-性能调优实例)
  - [1. CPU 高占用问题](#1-cpu-高占用问题)
  - [2. 高性能硬件上的程序部署问题](#2-高性能硬件上的程序部署问题)
  - [3. 堆外内存导致的内存溢出问题](#3-堆外内存导致的内存溢出问题)
  - [4. 外部命令导致系统卡顿问题](#4-外部命令导致系统卡顿问题)
  - [6. Windows 虚拟内存导致卡顿问题](#6-windows-虚拟内存导致卡顿问题)
  - [7. Refer Links](#7-refer-links)

# JVM 性能调优实例

## 1. CPU 高占用问题

[关于 JVM CPU 资源占用过高的问题排查](https://my.oschina.net/shipley/blog/52006)2

[线上 java 程序 CPU 占用过高问题排查](https://blog.csdn.net/u010862794/article/details/78020231)

1. 通过 top 命令查看当前 CPU 情况，找到 CPU 占用高的进程 PID
1. 获取 `top -H -p<pid>` 或 `ps -mp <pid> -o THREAD,tid,time` 获取该进程的各个线程的执行情况，找到 CPU 占用高的线程 tid（并通过 `printf "%x\n" <tid>` 转化为十六进制）
1. 通过 `jstack -l <pid> | grep <tid>` 命令获取当前进程的线程栈，并在其中找到上一步的线程 tid（十六进制）的对应代码段
1. 查看对应代码，从代码层面分析是否存在不合理逻辑

## 2. 高性能硬件上的程序部署问题

- 案例情况

  一个 15 万 PV / 天左右的在线文档类型网站最近更换了硬件系统，新的硬件为 4 个 CPU、16GB 物理内存，操作系统为 64 位 CentOS 5.4, Resin 作为 Web 服务器。整个服务器暂时没有部署别的应用，所有硬件资源都可以提供给这访问量并不算太大的网站使用。管理员为了尽量利用硬件资源选用了 64 位的 JDK 1.5 , 并通过 `-Xmx` 和 `-Xms` 参数将 Java 堆固定在 12GB。使用一段时间后发现使用效果并不理想，网站经常不定期出现长时间失去响应的情况。

- 问题分析

  抛开程序代码实现上的问题，**在程序部署上的主要问题显然是过大的堆内存进行回收时带来的长时间的停顿**。但如果直接重新缩小给 Java 堆分配的内存，那么在硬件上的投资就显得很浪费。

  在高性能硬件上部署程序，一般有 2 种方式：
  - 通过 64 位 JDK 来使用大内存。

    使用这种部署方案，存在以下问题：
    - 内存回收导致的长时间停顿
    - 现阶段，64 位 JDK 的性能测试结果普遍低于 32 位 JDK
    - 需要保证程序足够稳定。因为这种应用要是产生堆溢出几乎就无法产生堆转储快照（Dump 文件将会达到十几 GB 乃至更大），即使产生了快照也几乎无法进行分析
    - 相同程序在 64 位 JDK 消耗的内存一般比 32 位 JDK 大，这是由于指针膨胀以及数据类型对齐补白等因素导致的

    对于用户交互性强、对停顿时间敏感的系统，**可以给 Java 虚拟机分配超大堆的前提是有把握把应用程序的 Full GC 频率控制得足够低，至少要低到不会影响用户使用**，譬如十几个小时乃至一天才出现一次 Full GC，这样可以通过在深夜执行定时任务的方式触发 Full GC 甚至自动重启应用服务器来保持内存可用空间在一个稳定的水平。**而控制 Full GC 频率的关键是看应用中绝大多数对象能否符合“朝生夕灭”的原则**，即大多数对象的生存时间不应太长，尤其是不能有成批量的、长生存时间的大对象产生，这样才能保障老年代空间的稳定。只要代码写得合理，应当都能实现在超大堆中正常使用而没有 Full GC，这样使用超大堆内存时，网站响应速度才会比较有保证。

    此案例中的管理员就采用了这一种部署方式。但是，程序代码的实现并不能有效地控制 Full GC 频率，因此导致了长时间的 GC 停顿问题。

  - 使用若干个 32 位虚拟机建立逻辑集群来利用硬件资源。具体做法是在一台物理机器上启动多个应用服务器进程，每个服务器进程分配不同端口，然后在前端搭建一个负载均衡器，以反向代理的方式来分配访问请求。

    使用这种部署方案，存在以下问题：
    - 尽量避免节点竞争全局的资源，最典型的就是磁盘竞争，各个节点如果同时访问某个磁盘文件的话（尤其是并发写操作容易出现问题)，很容易导致 IO 异常。
    - 很难最高效率地利用某些资源池，譬如连接池，一般都是在各个节点建立自己独立的连接池，这样有可能导致一些节点池满了而另外一些节点仍有较多空余。尽管可以使用集中式的 JNDI，但这个有一定复杂性并且可能带来额外的性能开销。
    - 各个节点仍然不可避免地受到 32 位的内存限制，在 32 位 Windows 平台中每个进程只能使用 2GB 的内存，考虑到堆以外的内存开销，堆一般最多只能开到 1.5GB。在某些 Linux 或 UNIX 系统（如 Solaris）中，可以提升到 3GB 乃至接近 4GB 的内存，但 32 位中仍然受最高 4GB(232) 内存的限制。
    - 大量使用本地缓存（如大量使用 HashMap 作为 K/V 缓存）的应用，在逻辑集群中会造成较大的内存浪费，因为每个逻辑节点上都有一份缓存，这时候可以考虑把本地缓存改为集中式缓存。

- 解决方案

  - 部署方案调整为建立 5 个 32 位 JDK 的逻辑集群，每个进程按 2GB 内存计算（其中堆固定为 1.5GB）, 占用了 10GB 内存。另外建立一个 Apache 服务作为前端均衡代理访问门户。
  - 考虑到用户对响应速度比较关心，并且文档服务的主要压力集中在磁盘和内存访问，CPU 资源敏感度较低，因此改为 CMS 收集器进行垃圾回收。

  部署方式调整后，服务再没有出现长时间停顿，速度比硬件升级前有较大提升。

## 3. 堆外内存导致的内存溢出问题

- 案例情况

  一个学校的小型项目：基于 B/S 的电子考试系统，为了实现客户端能实时地从服务器端接收考试数据，系统使用了逆向 AJAX 技术（也称为 Comet 或者 Server Side Push），选用 CometD 1.1.1 作为服务端推送框架，服务器是 Jetty 7.1.4，硬件为一台普通 PC 机，Core i5 CPU，4GB 内存，运行 32 位 Windows 操作系统。

  测试期间发现服务端不定时拋出内存溢出异常，网站管理员尝试过把堆开到最大，而 32 位系统最多到 1.6GB 就基本无法再加大了，而且开大了基本没效果，拋出内存溢出异常好像还更加频繁了。在内存溢出后从系统日志中找到异常堆栈：
  ```
  [org.eclipse.jetty.util.log]handle failed java.lang.OutOfMemoryError:null at sun.raise.Unsafe.allocateMemory (Native Method )
  at java.nio.DirectByteBuffer.<init> (DirectByteBuffer.java :99 )
  at java.nio.ByteBuffer.allocateDirect (ByteBuffer.java :288 )
  at org.eclipse.jetty.io.nio.DirectNIOBuffer.<init>
  ...
  ```

- 问题分析

  从日志中可知，DirectByteBuffer 也就是堆外直接内存溢出了。

  操作系统对每个进程能管理的内存是有限制的。这台服务器使用的 32 位 Windows 平台的限制是 2GB，其中划了 1.6GB 给 Java 堆，因此 Direct Memory 最大也只能在剩余的 0.4GB 空间中分出一部分。在此应用中导致溢出的关键是：垃圾收集进行时，虚拟机虽然会对 Direct Memory 进行回收，但是 Direct Memory 却不能像新生代、老年代那样发现空间不足了就通知收集器进行垃圾回收，它只能等待老年代满了后 Full GC，然后“顺便地”帮它清理掉内存的废弃对象。否则它只能一直等到拋出内存溢出异常时，先 catch 掉，再在 catch 块里面通过 `System.gc()` 进行回收。要是虚拟机还是不听（譬如打开了 `-XX:+DisableExplicitGC` 开关），那就只能眼睁睁地看着堆中还有许多空闲内存，自己却不得不拋出内存溢出异常了。

  而本案例中使用的 CometD 1.1.1 框架，正好有大量的 NIO 操作需要使用到 Direct Memory 内存，从而导致 Direct Memory 内存溢出。

- 解决方案

  通过 `-XX:MaxDirectMemorySize` 调整堆外直接内存大小。

## 4. 外部命令导致系统卡顿问题

- 案例情况

  一个数字校园应用系统在做大并发压力测试的时候，发现请求响应时间比较慢，通过操作系统的 mpstat 工具发现 CPU 使用率很高，且系统占用绝大多数的 CPU 资源的程序并不是应用系统本身。这是个不正常的现象，通常情况下用户应用的 CPU 占用率应该占主要地位才能说明系统是正常工作的。

  通过 Solaris 10 的 Dtrace 脚本可以查看当前情况下哪些系统调用花费了最多的 CPU 资源，Dtrace 运行后发现最消耗 CPU 资源的竟然是 `fork` 系统调用。`fork` 系统调用是 Linux 用来产生新进程的，在 Java 虚拟机中，而用户编写的 Java 代码最多只有线程的概念，不应当有进程的产生。

- 问题分析

  通过本系统的开发人员排查，最终找到了答案：每个用户请求的处理都需要执行一个外部 shell 脚本来获得系统的一些信息。执行这个 shell 脚本是通过 Java 的 `Runtime.getRuntime().exec()` 方法来调用的。Java 虚拟机执行这个命令的过程是：首先克隆一个和当前虚拟机拥有一样环境变量的进程，再用这个新的进程去执行外部命令，最后再退出这个进程。虽然这种调用方式可以达到目的，但是它在 Java 虚拟机中是非常消耗资源的操作，即使外部命令本身能很快执行完毕，频繁调用时系统的消耗会很大，不仅是 CPU, 内存负担也很重。

- 解决方案

  去掉这个 Shell 脚本执行的语句，改为使用 Java 的 API 去获取这些信息后，系统很快恢复了正常。

## 6. Windows 虚拟内存导致卡顿问题

- 案例情况

  有一个带心跳检测功能的 GUI 桌面程序上线后发现心跳检测有误报的概率，查询日志发现误报的原因是程序会偶尔出现间隔约一分钟左右的时间完全无日志输出，处于停顿状态。

- 问题分析

  因为是桌面程序，所需的内存并不大 (`-Xmx256m`)，所以开始并没有想到是 GC 导致的程序停顿，但是加入參数 `-XX:+PrintGCApplicationStoppedTime -XX:+PrintGCDateStamps -Xloggc:gclog.log` 后，从 GC 日志文件中确认了停顿确实是由 GC 导致的，大部分 GC 时间都控制在 100 毫秒以内，但偶尔就会出现一次接近 1 分钟的 GC。从日志中可以看出，真正执行 GC 动作的时间不是很长，但从准备开始 GC 到真正开始 GC 之间所消耗的时间却占了绝大部分。

  除 GC 日志之外，还观察到这个 GUI 程序内存变化的一个特点：当它最小化的时候，资源管理中显示的占用内存大幅度减小，但是虚拟内存则没有变化。在 MSDN 上查证后，确定是程序在最小化时它的工作内存会被自动交换到磁盘的页面文件之中，导致发生 GC 时因为恢复页面文件的操作而导致不正常的 GC 停顿。

- 解决方案

  在 Java 的 GUI 程序中要避免这种现象，可以加入参数 `-Dsun.awt.keepWorkingSetOnMinimize=true` 来解决。这个参数在许多 AWT 的程序上都有应用，例如 JDK 自带的 Visual VM，用于保证程序在恢复最小化时能够立即响应。

## 7. Refer Links

[深入理解 Java 虚拟机 - 第五章调优案例分析与实战](https://blog.csdn.net/coslay/article/details/48950677)

[jvm 系列（六):Java 服务 GC 参数调优案例](http://www.ityouknow.com/jvm/2017/09/19/GC-tuning.html)
