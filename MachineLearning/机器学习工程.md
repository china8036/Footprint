- [机器学习工程](#机器学习工程)
  - [1. 样本工程](#1-样本工程)
  - [2. 特征工程](#2-特征工程)
  - [3. 训练工程](#3-训练工程)
  - [4. 线上工程](#4-线上工程)

# 机器学习工程

机器学习方法已经被广泛应用到工业界。机器学习相关工程也越来越重要。其要求工程人员具备机器学习的基础知识，拥有将机器学习算法在实际业务中落地的能力。

## 1. 样本工程

样本工程通常分为两步：

根据业务数据，构造原始样本。以推荐工程为例子，就是需要通过曝光和点击日志，构造用户对曝光内容反馈的正例和负例（点 or 不点 or 点不喜欢）。
补充原始样本中需要的原始特征。以上述样本为例，需要对每一条样本补充用户的性别、年龄、基础画像，点击内容的标题、分类、标签、统计信息等等。
第 1 个步骤需要在线上工程收集足够的信息，将其拼接在一起。第 2 个步骤需要使用用户基础数据，内容基础数据将样本数据补充完整。样本的构造方案有可能变化，需要的原始特征也有可能发生变化。

样本工程通常分为定时样本工程（天级别、小时级别) 和实时样本工程（分钟级别、秒级别），需要使用不同的数据计算架构实现。

## 2. 特征工程

在样本工程里，产出包含完整原始特征的样本后，需要将原始特征转换为数学表示（例如将城市文本，编号为数字，转为 one-hot 向量）。特征制作过程中有可能产生一些交叉特征，例如将特征“城市 + 用户年龄”合起来进行编码。

特征工程是需要算法研究院和数据工程师共同参与的，并且需要严格保证线下特征（用于训练）与线上特征（用于预测）的一致性。

更进一步，可以抽象公共的特征，建立特征库，减少算法研究在特征工程少消耗的时间，加快算法迭代。

## 3. 训练工程

感谢 Tensorflow、PyTorch 等开源机器学习训练框架，极大降低了机器学习的代码开发难度，增进了机器学习的开源协同、模型共享。

但是大规模数据的机器学习框架，一直都是各大公司藏而不宣的秘密武器。现在主流的方法有两种，一是单机多卡（GPU）的超级计算机；一是多机联合的分布式训练框架。

分布式 / 多卡训练架构主要有两种，一是 Parameter Server，即训练节点与参数服务器进行参数的通信和同步，咱们公司的无量就是采用的 PS 架构。另一种是 Ring-allreduce，训练节点成环状网络。百度的 PaddlePaddle 平台就引入了 Ring-allreduce。

实现分布式训练框架，不但要有良好的工程水平，还需要对分布式训练的算法细节深入了解；需要开发人员同时具备机器学习和工程的专业知识，对开发人员的知识、技能水平要求颇高。机器学习训练工程开发人才，是行业内的稀有人才。

## 4. 线上工程

数据规模的增大，和特征空间的增大，都给线上预测带来了较大难度。因为线上服务器往往不能像离线训练那样使用超级计算机，预测的性能、单机内存 /CPU 等瓶颈、模型的更新迭代都是大型模型在业务落地的重大挑战。

计算能力、计算成本的限制，使得很多场景无法 100% 使用深度模型，而是采取深度模型 + 传统模型共同服务的方案。

面对如此困局，工程人员应该在平衡成本和收益的前提下，去支持尽可能大、尽可能深的模型。减少工程对算法的限制，给予算法更多的探索空间。
