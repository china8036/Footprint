
## 1. 参数

[机器学习中的参数与超参数之间的区别](https://blog.csdn.net/shenxiaoming77/article/details/76849929)

[什么是超参数](https://blog.csdn.net/xiewenbo/article/details/51585054)

参数作为模型从历史训练数据中学到的一部分，是机器学习算法的关键。

### 1.1. 模型参数 (model parameter)

模型参数是模型内部的配置变量，可以用数据估计模型参数的值，即一种模型可以根据数据可以自动学习出的变量。

模型参数有以下特征：
- 进行模型预测时需要模型参数。
- 模型参数值可以定义模型功能。
- 模型参数用数据估计或数据学习得到。
- 模型参数一般不由实践者手动设置。
- 模型参数通常作为学习模型的一部分保存。

通常使用优化算法估计模型参数，优化算法是对参数的可能值进行的一种有效搜索。

e.g.
- 人造神经网络中的权重
- 支持向量机中的支持向量
- 线性回归或逻辑回归中的系数

### 1.2. 模型超参数 (model Hyperparameter)

[模型超参数](https://en.wikipedia.org/wiki/Hyperparameter) 是模型外部的配置变量，其值不能从数据估计得到，必须手动设置参数的值，超参数不同，模型也是不同的。

模型超参数的具体特征有：
- 模型超参数常应用于估计模型参数的过程中。
- 模型超参数通常由实践者直接指定。
- 模型超参数通常可以使用启发式方法来设置。
- 模型超参数通常根据给定的预测建模问题而调整。

对于给定的问题，我们无法知道模型超参数的最优值。但我们可以使用经验法则来探寻其最优值，或复制用于其他问题的值，也可以通过反复试验的方法。

超参数通常是手工设定，不断试错调整，或者对一系列穷举出来的参数组合一通枚举（叫做网格搜索）。深度学习和神经网络模型，有很多这样的参数需要学习，这就是为什么过去这么多年从业者弃之不顾的原因。以前给人的印象，深度学习就是“黑魔法”。时至今日，非参数学习研究正在帮助深度学习更加自动的优化模型参数选择（AutoML），当然有经验的专家仍然是必须的。

e.g.
- 训练神经网络的学习速率、迭代次数、层数、每层神经元的个数等。
- 支持向量机的 C 和 sigma 超参数
- k 邻域中的 k

## 2. batchsize

TODO:

[深度学习中的 batch 的大小对学习效果有何影响？](https://www.zhihu.com/question/32673260/answer/71137399)

[深度学习中的 batch 的大小对学习效果有何影响？](https://www.zhihu.com/question/32673260)

[神经网络训练中，傻傻分不清 Epoch、Batch Size 和迭代](https://www.jiqizhixin.com/articles/2017-09-25-3)

[怎么选取训练神经网络时的 Batch size?](https://www.zhihu.com/question/61607442)

[深度学习中的 batch 的大小对学习效果有何影响？](https://www.zhihu.com/question/32673260/answer/71137399)

### 2.1. 基本概念

- batchsize：批大小。在深度学习中，一般采用 SGD 训练，即每次训练在训练集中取 batchsize 个样本训练；
- iteration：1 个 iteration 等于使用 batchsize 个样本训练一次；
- epoch：1 个 epoch 等于使用训练集中的全部样本训练一次；

e.g.

训练的 dataset 有 1000 个样本，batchsize=10，那么训练完整个样本集需要：
- 100 次 iteration
- 1 次 epoch

### 2.2. 存在意义

Batch 的选择，首先决定的是下降的方向。

- Full Batch Learning

  如果数据集比较小，完全可以采用全数据集 （ Full Batch Learning ）的形式，这样做至少有 2 个好处：
  - 由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。
  - 由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 Full Batch Learning 可以使用 Rprop 只基于梯度符号并且针对性单独更新各权值。

  对于更大的数据集，以上 2 个好处又变成了 2 个坏处：
  - 随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。
  - 以 Rprop 的方式迭代，会由于各个 Batch 之间的采样差异性，各次梯度修正值相互抵消，无法修正。这才有了后来 RMSProp 的妥协方案。

- Batch_Size = 1

  既然 Full Batch Learning 并不适用大数据集，那么走向另一个极端怎么样？

  每次只训练一个样本，即 Batch_Size = 1，这就是在线学习（Online Learning）。线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元、非线性网络，在局部依然近似是抛物面。使用在线学习，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，**难以达到收敛**。

- 合理选择 Batch_Size

  可不可以选择一个适中的 Batch_Size 值呢？

  当然可以，这就是批梯度下降法（Mini-batches Learning）。因为如果数据集足够充分，那么用一半（甚至少得多）的数据训练算出来的梯度与用全部数据训练出来的梯度是几乎一样的。

  Batch_Size 将影响到模型的优化程度和速度，Batch_Size 的正确选择能够在**内存效率和内存容量**之间找到最佳平衡。

  - 合理增大 Batch_Size 有何好处？
    - 内存利用率提高了，大矩阵乘法的并行化效率提高。
    - 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。
    - 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。

  - 盲目增大 Batch_Size 有何坏处？
    - 内存利用率提高了，但是内存容量可能撑不住了。
    - 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。
    - Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。

适当的增加 Batchsize 的优点：
- 通过并行化提高内存利用率。
- 单次 epoch 的迭代次数减少，提高运行速度（单次 epoch=（全部训练样本 /batchsize） / iteration = 1）。
- 适当的增加 Batch_Size，梯度下降方向准确度增加，训练震动的幅度减小。

## 3. Refer Links
